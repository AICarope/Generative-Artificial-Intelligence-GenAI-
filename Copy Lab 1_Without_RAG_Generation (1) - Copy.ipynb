{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vMCaTzdu6a"
      },
      "source": [
        "# Welcome to Lab-1\n",
        "## In this Notebook we will learn how we can use different documents and get them analysed by GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VROza_wGd4if"
      },
      "source": [
        "## First of all we will install the required packages that will help us along the way\n",
        "1. openai: This package will help us to call the chat completion method of openai to generate results using GPT.\n",
        "3. PyMuPDF: This package is used for easy PDF manipulation.\n",
        "4. tiktoken: This package is used to calculate the tokens in a text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cgny4E8V8NGU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai==1.3.9 PyMuPDF==1.24.2 PyMuPDFb==1.24.1 tqdm tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H6Z8_eO38qlS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tiktoken\n",
        "\n",
        "# As described in prerequiste step 1. Replace your OpenAPI key here.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-QbKUMBHI5REdskyFmXTjT3BlbkFJWtvYGJqlR93rpY9UCRps\"\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErbsjVzCgLcG"
      },
      "source": [
        "Here we have created a function that is called for generating response from OpenAI\n",
        "\n",
        "This loads the keys from the environment and uses it to call the `openai.chat.completions.create` method\n",
        "\n",
        "This method takes the system message and the user message as input for generating response.\n",
        "\n",
        "### The temperature parameter defines the randomness of the output,\n",
        "\n",
        "Higher the temperature the more creative the LLM will become with its answers, thus higher temperatures are used for poem generation, jokes etc.\n",
        "Lower temperature gives deterministic results and return the most probable next token.\n",
        "\n",
        "### Top P\n",
        "\n",
        "\n",
        "Imagine you have a bag of words the model can choose from. Nucleus sampling is like picking a smaller bag of the most likely words. A small bag (low value) gives fewer choices, making answers more exact but less creative. A bigger bag (high value) gives more choices, making answers more surprising but less reliable.\n",
        "\n",
        "Top P is similar, but instead of bag size, it controls how many words you consider \"good enough\"  A low value only picks the absolute best words, leading to safe but boring answers.  A high value includes some less likely words, making the answers more interesting but potentially less accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UXJBE_y19pCq"
      },
      "outputs": [],
      "source": [
        "def CallOpenAI(user,system):\n",
        "  response = openai.chat.completions.create(\n",
        "              model= model_name, # model = \"deployment_name\".\n",
        "              temperature= 0,\n",
        "              top_p= 0,\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": system},\n",
        "                  {\"role\": \"user\", \"content\": user}\n",
        "              ]\n",
        "          )\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2YvMRDm-Fx5"
      },
      "source": [
        "## Lets take a contract and try to analyse it without much instruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JwKDwhgpvm"
      },
      "source": [
        "First we load the PDF and extract the texts from it and generate the token count of the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sI9uVaob_7Ie"
      },
      "outputs": [],
      "source": [
        "def extract_text(pdf_path):\n",
        "  pdf = fitz.open(pdf_path)\n",
        "  text = ''\n",
        "\n",
        "  for page in pdf:\n",
        "    text += page.get_text()\n",
        "\n",
        "  num_tokens = len(token_encoding.encode(text))\n",
        "  print(\"Number of tokens in the entire Document: \", num_tokens)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtScsyS8gz0r"
      },
      "source": [
        "Out here we can see the token count of the document is 11590 which is well withing the 16000 context limit of the GPT-3.5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opozpCU64wW9"
      },
      "source": [
        "Also upload the .PDF in the Colab runtime.\n",
        "\n",
        "Follow the Instructions given in Lab-0 to where we have shown how to upload the files to the Colab runtime.\n",
        "\n",
        "Here's the link:\n",
        "\n",
        "https://github.com/initmahesh/MLAI-community-labs/blob/main/Class-Labs/Lab-0(Pre-requisites)/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-vel8SEHS_C",
        "outputId": "665bc38a-e7c5-42df-cb2a-1c7d1a742591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in the entire Document:  11590\n"
          ]
        }
      ],
      "source": [
        "short_document = extract_text(\"AWS1.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtnxFpZyg9gr"
      },
      "source": [
        "## We concatenate the text from the PDF and the question that the user wants to ask to the GPT about the PDF and form a prompt that we will use to generate the response using `openai.chat.completion.create` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PJlC7zoDHmpV"
      },
      "outputs": [],
      "source": [
        "Question = \"What is the governing law for Amazon Web Services South Africa ProprietaryLimited\"\n",
        "\n",
        "full_prompt_SD = \"<Context>\"+short_document+\"</Context>\" +\"\\n\\n\" +\"<Question>\"+Question+\"</Question>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxWlkJ_LFcmu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "097gI1cFKWHu"
      },
      "outputs": [
        {
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************CRps. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Carmen\\Documents\\PreMaster\\ARTIFICIAL INTELLIGENCE\\AI PM FAANG\\Labs\\Lab 0 1 2\\Copy Lab 1_Without_RAG_Generation (1) - Copy.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m CallOpenAI(full_prompt_SD,\u001b[39m\"\u001b[39m\u001b[39mYou are a Professional lawyer who can analyse documents thorougly\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\Carmen\\Documents\\PreMaster\\ARTIFICIAL INTELLIGENCE\\AI PM FAANG\\Labs\\Lab 0 1 2\\Copy Lab 1_Without_RAG_Generation (1) - Copy.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mCallOpenAI\u001b[39m(user,system):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m               model\u001b[39m=\u001b[39m model_name, \u001b[39m# model = \"deployment_name\".\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               temperature\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m               top_p\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m               messages\u001b[39m=\u001b[39m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                   {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: system},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                   {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: user}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m               ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m           )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Carmen/Documents/PreMaster/ARTIFICIAL%20INTELLIGENCE/AI%20PM%20FAANG/Labs/Lab%200%201%202/Copy%20Lab%201_Without_RAG_Generation%20%281%29%20-%20Copy.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 303\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[0;32m    599\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/chat/completions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    600\u001b[0m         body\u001b[39m=\u001b[39mmaybe_transform(\n\u001b[0;32m    601\u001b[0m             {\n\u001b[0;32m    602\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: messages,\n\u001b[0;32m    603\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[0;32m    604\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfrequency_penalty\u001b[39m\u001b[39m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    605\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m: function_call,\n\u001b[0;32m    606\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunctions\u001b[39m\u001b[39m\"\u001b[39m: functions,\n\u001b[0;32m    607\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogit_bias\u001b[39m\u001b[39m\"\u001b[39m: logit_bias,\n\u001b[0;32m    608\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_tokens,\n\u001b[0;32m    609\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[0;32m    610\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpresence_penalty\u001b[39m\u001b[39m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    611\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mresponse_format\u001b[39m\u001b[39m\"\u001b[39m: response_format,\n\u001b[0;32m    612\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m: seed,\n\u001b[0;32m    613\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop,\n\u001b[0;32m    614\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream,\n\u001b[0;32m    615\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[0;32m    616\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtool_choice\u001b[39m\u001b[39m\"\u001b[39m: tool_choice,\n\u001b[0;32m    617\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtools\u001b[39m\u001b[39m\"\u001b[39m: tools,\n\u001b[0;32m    618\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[0;32m    619\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: user,\n\u001b[0;32m    620\u001b[0m             },\n\u001b[0;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    622\u001b[0m         ),\n\u001b[0;32m    623\u001b[0m         options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[0;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39mextra_headers, extra_query\u001b[39m=\u001b[39mextra_query, extra_body\u001b[39m=\u001b[39mextra_body, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[0;32m    625\u001b[0m         ),\n\u001b[0;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39mChatCompletion,\n\u001b[0;32m    627\u001b[0m         stream\u001b[39m=\u001b[39mstream \u001b[39mor\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    629\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(cast_to, opts, stream\u001b[39m=\u001b[39mstream, stream_cls\u001b[39m=\u001b[39mstream_cls))\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[0;32m    854\u001b[0m         cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m    855\u001b[0m         options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m    856\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[0;32m    857\u001b[0m         stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[0;32m    858\u001b[0m         remaining_retries\u001b[39m=\u001b[39mremaining_retries,\n\u001b[0;32m    859\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
            "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************CRps. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "response = CallOpenAI(full_prompt_SD,\"You are a Professional lawyer who can analyse documents thorougly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT-ynR-nhSdx"
      },
      "source": [
        "### We can see that the GPT was able to generate the answer by refering to the prompt and give the correct result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZjJRGvhK_Ra",
        "outputId": "3ed593ea-17ce-42bc-bc74-fd4daa7c71b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The governing law for Amazon Web Services South Africa Proprietary Limited is the laws of the Republic of South Africa. The courts that would have jurisdiction over any disputes related to this entity would be The South Gauteng High Court in Johannesburg.\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_rAndtLKzTq"
      },
      "source": [
        "## Now lets load up a document that has more than 16000 tokens, which is the limit of GPT-3.5-Turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_ovyUE66ko7"
      },
      "source": [
        "Upload the .PDF in the Colab runtime.\n",
        "\n",
        "Follow the Instructions given in Lab-0 to where we have shown how to upload the files to the Colab runtime.\n",
        "\n",
        "Here's the link:\n",
        "\n",
        "https://github.com/initmahesh/MLAI-community-labs/blob/main/Class-Labs/Lab-0(Pre-requisites)/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja6FHWxeAiKo",
        "outputId": "78e15ec1-bdc8-41c1-e0a8-00c7bfaf2586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in the entire Document:  163227\n"
          ]
        }
      ],
      "source": [
        "long_document = extract_text(\"PROFRAC HOLDINGS, LLC credit agreement.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfpmAVqmKCXM"
      },
      "outputs": [],
      "source": [
        "Question = \"What is the Acknowledgement Regarding Any Supported QFCs?\"\n",
        "\n",
        "full_prompt_LD = \"<Context>\"+long_document+\"</Context>\" +\"\\n\\n\" +\"<Question>\"+Question+\"</Question>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve7bc33tLygQ"
      },
      "source": [
        "## Here what you see is, when the message length exceeded the limit of GPT, it throws an error.\n",
        "### This problem will be fixed in the next lab where you see how Retrieval Augmented Generation(RAG) will fix this problem and enable us to analyse documents of any length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBWHfMohKxKw",
        "outputId": "98a782f3-bbd1-4dfe-eeae-f00adc1f44a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context length is more than the input capacity of 16000\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  response = CallOpenAI(full_prompt_LD,\"You are a Professional lawyer who can analyse documents thorougly\")\n",
        "except Exception as e:\n",
        "  print(\"Context length is more than the input capacity of 16000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Hvd7SzkwUA"
      },
      "source": [
        "## Lets discuss this in next lab as how we solve this problem with RAG(Retrival augmented generation)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
