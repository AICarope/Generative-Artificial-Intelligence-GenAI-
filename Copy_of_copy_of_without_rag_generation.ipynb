{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vMCaTzdu6a"
      },
      "source": [
        "# Welcome to Lab-1\n",
        "## In this Notebook we will learn how we can use different documents and get them analysed by GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VROza_wGd4if"
      },
      "source": [
        "## First of all we will install the required packages that will help us along the way\n",
        "1. openai: This package will help us to call the chat completion method of openai to generate results using GPT.\n",
        "3. PyMuPDF: This package is used for easy PDF manipulation.\n",
        "4. tiktoken: This package is used to calculate the tokens in a text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgny4E8V8NGU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai==1.3.9 PyMuPDF==1.24.2 PyMuPDFb==1.24.1 tqdm tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6Z8_eO38qlS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tiktoken\n",
        "\n",
        "# As described in prerequiste step 1. Replace your OpenAPI key here.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-mzypCstrTOuG6zzxNyEcT3BlbkFJKwABYtAKmNv6gQfHQCrps\"\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErbsjVzCgLcG"
      },
      "source": [
        "Here we have created a function that is called for generating response from OpenAI\n",
        "\n",
        "This loads the keys from the environment and uses it to call the `openai.chat.completions.create` method\n",
        "\n",
        "This method takes the system message and the user message as input for generating response.\n",
        "\n",
        "### The temperature parameter defines the randomness of the output,\n",
        "\n",
        "Higher the temperature the more creative the LLM will become with its answers, thus higher temperatures are used for poem generation, jokes etc.\n",
        "Lower temperature gives deterministic results and return the most probable next token.\n",
        "\n",
        "### Top P\n",
        "\n",
        "\n",
        "Imagine you have a bag of words the model can choose from. Nucleus sampling is like picking a smaller bag of the most likely words. A small bag (low value) gives fewer choices, making answers more exact but less creative. A bigger bag (high value) gives more choices, making answers more surprising but less reliable.\n",
        "\n",
        "Top P is similar, but instead of bag size, it controls how many words you consider \"good enough\"  A low value only picks the absolute best words, leading to safe but boring answers.  A high value includes some less likely words, making the answers more interesting but potentially less accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXJBE_y19pCq"
      },
      "outputs": [],
      "source": [
        "def CallOpenAI(user,system):\n",
        "  response = openai.chat.completions.create(\n",
        "              model= model_name, # model = \"deployment_name\".\n",
        "              temperature= 0,\n",
        "              top_p= 0,\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": system},\n",
        "                  {\"role\": \"user\", \"content\": user}\n",
        "              ]\n",
        "          )\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5G74CJlKP0v9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZA4IQkA1P1Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2YvMRDm-Fx5"
      },
      "source": [
        "## Lets take a contract and try to analyse it without much instruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JwKDwhgpvm"
      },
      "source": [
        "First we load the PDF and extract the texts from it and generate the token count of the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI9uVaob_7Ie"
      },
      "outputs": [],
      "source": [
        "def extract_text(pdf_path):\n",
        "  pdf = fitz.open(pdf_path)\n",
        "  text = ''\n",
        "\n",
        "  for page in pdf:\n",
        "    text += page.get_text()\n",
        "\n",
        "  num_tokens = len(token_encoding.encode(text))\n",
        "  print(\"Number of tokens in the entire Document: \", num_tokens)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtScsyS8gz0r"
      },
      "source": [
        "Out here we can see the token count of the document is 11590 which is well withing the 16000 context limit of the GPT-3.5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also upload the .PDF in the Colab runtime.\n",
        "\n",
        "Follow the Instructions given in Lab-0 to where we have shown how to upload the files to the Colab runtime.\n",
        "\n",
        "Here's the link:\n",
        "\n",
        "https://github.com/initmahesh/MLAI-community-labs/blob/main/Class-Labs/Lab-0(Pre-requisites)/README.md"
      ],
      "metadata": {
        "id": "opozpCU64wW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-vel8SEHS_C",
        "outputId": "76c0a088-df5f-4ae9-a088-151d4cc928b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the entire Document:  11590\n"
          ]
        }
      ],
      "source": [
        "short_document = extract_text(\"AWS1.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtnxFpZyg9gr"
      },
      "source": [
        "## We concatenate the text from the PDF and the question that the user wants to ask to the GPT about the PDF and form a prompt that we will use to generate the response using `openai.chat.completion.create` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJlC7zoDHmpV"
      },
      "outputs": [],
      "source": [
        "Question = \"What is the governing law for Amazon Web Services South Africa ProprietaryLimited\"\n",
        "\n",
        "full_prompt_SD = \"<Context>\"+short_document+\"</Context>\" +\"\\n\\n\" +\"<Question>\"+Question+\"</Question>\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LxWlkJ_LFcmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "097gI1cFKWHu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "f7ebf615-b228-4921-891d-2f8f4b24b8ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-*********************************************Vht). You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2bc6e238010a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_prompt_SD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"You are a Professional lawyer who can analyse documents thorougly\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-fc8f8f0eda13>\u001b[0m in \u001b[0;36mCallOpenAI\u001b[0;34m(user, system)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCallOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# model = \"deployment_name\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m         )\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 853\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-*********************************************Vht). You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "response = CallOpenAI(full_prompt_SD,\"You are a Professional lawyer who can analyse documents thorougly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT-ynR-nhSdx"
      },
      "source": [
        "### We can see that the GPT was able to generate the answer by refering to the prompt and give the correct result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "2ZjJRGvhK_Ra",
        "outputId": "5bd447de-366d-4ffa-f6ab-83610be1e573"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'response' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8f996d9adbdd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_rAndtLKzTq"
      },
      "source": [
        "## Now lets load up a document that has more than 16000 tokens, which is the limit of GPT-3.5-Turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the .PDF in the Colab runtime.\n",
        "\n",
        "Follow the Instructions given in Lab-0 to where we have shown how to upload the files to the Colab runtime.\n",
        "\n",
        "Here's the link:\n",
        "\n",
        "https://github.com/initmahesh/MLAI-community-labs/blob/main/Class-Labs/Lab-0(Pre-requisites)/README.md"
      ],
      "metadata": {
        "id": "J_ovyUE66ko7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja6FHWxeAiKo",
        "outputId": "d829e63b-bc8a-4d9a-ccdc-30a942367368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the entire Document:  163227\n"
          ]
        }
      ],
      "source": [
        "long_document = extract_text(\"PROFRAC HOLDINGS, LLC credit agreement.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfpmAVqmKCXM"
      },
      "outputs": [],
      "source": [
        "Question = \"What is the Acknowledgement Regarding Any Supported QFCs?\"\n",
        "\n",
        "full_prompt_LD = \"<Context>\"+long_document+\"</Context>\" +\"\\n\\n\" +\"<Question>\"+Question+\"</Question>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve7bc33tLygQ"
      },
      "source": [
        "## Here what you see is, when the message length exceeded the limit of GPT, it throws an error.\n",
        "### This problem will be fixed in the next lab where you see how Retrieval Augmented Generation(RAG) will fix this problem and enable us to analyse documents of any length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBWHfMohKxKw",
        "outputId": "9546547f-6bfb-4357-8698-7d3d8a0c2a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context length is more than the input capacity of 16000\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  response = CallOpenAI(full_prompt_LD,\"You are a Professional lawyer who can analyse documents thorougly\")\n",
        "except Exception as e:\n",
        "  print(\"Context length is more than the input capacity of 16000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets discuss this in next lab as how we solve this problem with RAG(Retrival augmented generation)"
      ],
      "metadata": {
        "id": "75Hvd7SzkwUA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}